{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc922ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (4.42.3)\n",
      "Requirement already satisfied: huggingface_hub in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (0.23.4)\n",
      "Requirement already satisfied: filelock in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17 in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from huggingface_hub) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from huggingface_hub) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2023.11.17)\n",
      "Requirement already satisfied: transformers[torch] in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (4.42.3)\n",
      "Requirement already satisfied: accelerate in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (0.32.1)\n",
      "Requirement already satisfied: filelock in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from transformers[torch]) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from transformers[torch]) (0.23.4)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17 in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from transformers[torch]) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from transformers[torch]) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from transformers[torch]) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from transformers[torch]) (2022.7.9)\n",
      "Requirement already satisfied: requests in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from transformers[torch]) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from transformers[torch]) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from transformers[torch]) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from transformers[torch]) (4.65.0)\n",
      "Requirement already satisfied: torch in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from transformers[torch]) (2.1.2)\n",
      "Requirement already satisfied: psutil in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (4.7.1)\n",
      "Requirement already satisfied: sympy in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from torch->transformers[torch]) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from torch->transformers[torch]) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from torch->transformers[torch]) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from requests->transformers[torch]) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from requests->transformers[torch]) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from requests->transformers[torch]) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from requests->transformers[torch]) (2023.11.17)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from jinja2->torch->transformers[torch]) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from sympy->torch->transformers[torch]) (1.3.0)\n",
      "Requirement already satisfied: transformers[torch] in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (4.42.3)\n",
      "Requirement already satisfied: filelock in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from transformers[torch]) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from transformers[torch]) (0.23.4)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17 in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from transformers[torch]) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from transformers[torch]) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from transformers[torch]) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from transformers[torch]) (2022.7.9)\n",
      "Requirement already satisfied: requests in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from transformers[torch]) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from transformers[torch]) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from transformers[torch]) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from transformers[torch]) (4.65.0)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from transformers[torch]) (0.32.1)\n",
      "Requirement already satisfied: torch in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from transformers[torch]) (2.1.2)\n",
      "Requirement already satisfied: psutil in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (4.7.1)\n",
      "Requirement already satisfied: sympy in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from torch->transformers[torch]) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from torch->transformers[torch]) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from torch->transformers[torch]) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from requests->transformers[torch]) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from requests->transformers[torch]) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from requests->transformers[torch]) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from requests->transformers[torch]) (2023.11.17)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from jinja2->torch->transformers[torch]) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from sympy->torch->transformers[torch]) (1.3.0)\n",
      "Requirement already satisfied: accelerate in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (0.32.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.17 in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from accelerate) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from accelerate) (23.1)\n",
      "Requirement already satisfied: psutil in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from accelerate) (2.1.2)\n",
      "Requirement already satisfied: huggingface-hub in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from accelerate) (0.23.4)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from accelerate) (0.4.3)\n",
      "Requirement already satisfied: filelock in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (4.7.1)\n",
      "Requirement already satisfied: sympy in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (2024.6.1)\n",
      "Requirement already satisfied: requests in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from huggingface-hub->accelerate) (4.65.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: sentencepiece in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (0.2.0)\n",
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: six in /Users/sauravsahu/anaconda3/lib/python3.11/site-packages (from langdetect) (1.16.0)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993225 sha256=511fd934977146dfa9af11da578d0b7def8649d134e6fcdbda559add84db56f8\n",
      "  Stored in directory: /Users/sauravsahu/Library/Caches/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect\n",
      "Successfully installed langdetect-1.0.9\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers huggingface_hub\n",
    "!pip install transformers[torch] accelerate\n",
    "!pip install transformers[torch]\n",
    "!pip install accelerate -U\n",
    "!pip install sentencepiece\n",
    "!pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a4e440",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "#pdf_path = 'EVB_Nissan_2013.pdf'\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc.load_page(page_num)\n",
    "        text += page.get_text()\n",
    "    doc.close()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89a2cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Example: remove special characters, extra spaces, etc.\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with single space\n",
    "    text = text.strip()  # Remove leading and trailing whitespace\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f6fc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = 'gpt2'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Example fine-tuning data (replace with your own preprocessed text)\n",
    "train_text = \"Your preprocessed text data here.\"\n",
    "\n",
    "# Tokenize input text\n",
    "input_ids = tokenizer.encode(train_text, return_tensors='pt')\n",
    "\n",
    "# Fine-tuning the model\n",
    "model.train()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "for epoch in range(3):\n",
    "    outputs = model(input_ids, labels=input_ids)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained('fine_tuned_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d5721f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example text generation\n",
    "model.eval()\n",
    "generated = model.generate(input_ids, max_length=100, num_return_sequences=1)\n",
    "decoded_text = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d8f7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('llam2modeltrial.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c05732c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d9d6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import re\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Function to chunk text into smaller parts\n",
    "def chunk_text(text, chunk_size=1024):\n",
    "    chunks = []\n",
    "    for i in range(0, len(text), chunk_size):\n",
    "        chunks.append(text[i:i+chunk_size])\n",
    "    return chunks\n",
    "\n",
    "# Step 1: Extract text from PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc.load_page(page_num)\n",
    "        text += page.get_text()\n",
    "    doc.close()\n",
    "    return text\n",
    "\n",
    "# Step 2: Preprocess text\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with single space\n",
    "    text = text.strip()  # Remove leading and trailing whitespace\n",
    "    return text\n",
    "\n",
    "# Step 3: Fine-tune GPT-2 model\n",
    "def fine_tune_gpt2(chunks):\n",
    "    # Load pre-trained model and tokenizer\n",
    "    model_name = 'gpt2'\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "    # Fine-tuning the model\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "    x=1\n",
    "    for epoch in range(100):  # Adjust number of epochs as needed\n",
    "        for chunk in chunks:\n",
    "            input_ids = tokenizer.encode(chunk, return_tensors='pt')\n",
    "            outputs = model(input_ids, labels=input_ids)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            print(f\"Epoch {x}, Loss: {loss.item()}\")\n",
    "            x=x+1\n",
    "\n",
    "    # Save the fine-tuned model\n",
    "    model.save_pretrained('fine_tuned_model')\n",
    "\n",
    "    return model\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace with your PDF file path\n",
    "    pdf_path = 'EVB_Nissan_2013.pdf'\n",
    "\n",
    "    # Step 1: Extract text from PDF\n",
    "    pdf_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "    # Step 2: Preprocess text\n",
    "    preprocessed_text = preprocess_text(pdf_text)\n",
    "\n",
    "    # Step 2.1: Chunk text into smaller parts\n",
    "    text_chunks = chunk_text(preprocessed_text)\n",
    "\n",
    "    # Step 3: Fine-tune GPT-2 model on text chunks\n",
    "    fine_tuned_model = fine_tune_gpt2(text_chunks)\n",
    "\n",
    "    # Optionally, you can use the fine-tuned model for text generation or other tasks\n",
    "    # Example: Generate text\n",
    "    # Note: Since we've chunked the text, adjust the generation accordingly\n",
    "    input_text = \"how to remove service plug? what are the warnings and dangers?\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "    generated = fine_tuned_model.generate(input_ids, max_length=100, num_return_sequences=1)\n",
    "    decoded_text = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "    print(decoded_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdc8eb28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 3.898451805114746\n",
      "Epoch 1, Loss: 3.7374026775360107\n",
      "Epoch 1, Loss: 3.380272388458252\n",
      "Epoch 1, Loss: 4.753809452056885\n",
      "Epoch 2, Loss: 3.6544206142425537\n",
      "Epoch 2, Loss: 3.5366458892822266\n",
      "Epoch 2, Loss: 3.333765745162964\n",
      "Epoch 2, Loss: 4.572977542877197\n",
      "Epoch 3, Loss: 3.2973923683166504\n",
      "Epoch 3, Loss: 3.3765604496002197\n",
      "Epoch 3, Loss: 3.19071888923645\n",
      "Epoch 3, Loss: 4.392301082611084\n",
      "Epoch 4, Loss: 3.403414487838745\n",
      "Epoch 4, Loss: 3.3559553623199463\n",
      "Epoch 4, Loss: 3.1444873809814453\n",
      "Epoch 4, Loss: 4.391643047332764\n",
      "Epoch 5, Loss: 3.3066000938415527\n",
      "Epoch 5, Loss: 3.2852113246917725\n",
      "Epoch 5, Loss: 3.091521739959717\n",
      "Epoch 5, Loss: 4.024720191955566\n",
      "Epoch 6, Loss: 3.12064528465271\n",
      "Epoch 6, Loss: 3.1831343173980713\n",
      "Epoch 6, Loss: 2.9268763065338135\n",
      "Epoch 6, Loss: 4.006988048553467\n",
      "Epoch 7, Loss: 3.131103992462158\n",
      "Epoch 7, Loss: 3.109229326248169\n",
      "Epoch 7, Loss: 2.890228033065796\n",
      "Epoch 7, Loss: 3.7274138927459717\n",
      "Epoch 8, Loss: 3.0838687419891357\n",
      "Epoch 8, Loss: 2.9221878051757812\n",
      "Epoch 8, Loss: 2.7502408027648926\n",
      "Epoch 8, Loss: 3.474825143814087\n",
      "Epoch 9, Loss: 3.0492780208587646\n",
      "Epoch 9, Loss: 2.9003098011016846\n",
      "Epoch 9, Loss: 2.6630990505218506\n",
      "Epoch 9, Loss: 3.339395046234131\n",
      "Epoch 10, Loss: 2.9453237056732178\n",
      "Epoch 10, Loss: 2.827021360397339\n",
      "Epoch 10, Loss: 2.7188193798065186\n",
      "Epoch 10, Loss: 3.32503604888916\n",
      "Epoch 11, Loss: 2.851590633392334\n",
      "Epoch 11, Loss: 2.7596514225006104\n",
      "Epoch 11, Loss: 2.538292646408081\n",
      "Epoch 11, Loss: 3.2183382511138916\n",
      "Epoch 12, Loss: 2.7833704948425293\n",
      "Epoch 12, Loss: 2.652247905731201\n",
      "Epoch 12, Loss: 2.3401577472686768\n",
      "Epoch 12, Loss: 3.183055877685547\n",
      "Epoch 13, Loss: 2.6724538803100586\n",
      "Epoch 13, Loss: 2.619736433029175\n",
      "Epoch 13, Loss: 2.454925298690796\n",
      "Epoch 13, Loss: 3.004610300064087\n",
      "Epoch 14, Loss: 2.613037109375\n",
      "Epoch 14, Loss: 2.581049919128418\n",
      "Epoch 14, Loss: 2.349548578262329\n",
      "Epoch 14, Loss: 2.713934898376465\n",
      "Epoch 15, Loss: 2.5733907222747803\n",
      "Epoch 15, Loss: 2.498462200164795\n",
      "Epoch 15, Loss: 2.3017356395721436\n",
      "Epoch 15, Loss: 2.6668477058410645\n",
      "Epoch 16, Loss: 2.431706428527832\n",
      "Epoch 16, Loss: 2.4075915813446045\n",
      "Epoch 16, Loss: 2.168426036834717\n",
      "Epoch 16, Loss: 2.6292717456817627\n",
      "Epoch 17, Loss: 2.4291443824768066\n",
      "Epoch 17, Loss: 2.3475234508514404\n",
      "Epoch 17, Loss: 2.1929879188537598\n",
      "Epoch 17, Loss: 2.3776509761810303\n",
      "Epoch 18, Loss: 2.4933364391326904\n",
      "Epoch 18, Loss: 2.295121192932129\n",
      "Epoch 18, Loss: 2.047309160232544\n",
      "Epoch 18, Loss: 2.378297805786133\n",
      "Epoch 19, Loss: 2.281010866165161\n",
      "Epoch 19, Loss: 2.2256734371185303\n",
      "Epoch 19, Loss: 2.0422332286834717\n",
      "Epoch 19, Loss: 2.240187883377075\n",
      "Epoch 20, Loss: 2.288870096206665\n",
      "Epoch 20, Loss: 2.153718948364258\n",
      "Epoch 20, Loss: 2.039202928543091\n",
      "Epoch 20, Loss: 1.8589383363723755\n",
      "Epoch 21, Loss: 2.174956798553467\n",
      "Epoch 21, Loss: 2.2078874111175537\n",
      "Epoch 21, Loss: 1.8690773248672485\n",
      "Epoch 21, Loss: 2.041487693786621\n",
      "Epoch 22, Loss: 2.1459312438964844\n",
      "Epoch 22, Loss: 2.0735721588134766\n",
      "Epoch 22, Loss: 1.8433411121368408\n",
      "Epoch 22, Loss: 1.8223775625228882\n",
      "Epoch 23, Loss: 2.024822473526001\n",
      "Epoch 23, Loss: 1.9607305526733398\n",
      "Epoch 23, Loss: 1.7704942226409912\n",
      "Epoch 23, Loss: 1.7795116901397705\n",
      "Epoch 24, Loss: 2.0406179428100586\n",
      "Epoch 24, Loss: 1.8980803489685059\n",
      "Epoch 24, Loss: 1.6941395998001099\n",
      "Epoch 24, Loss: 1.653387188911438\n",
      "Epoch 25, Loss: 1.9546098709106445\n",
      "Epoch 25, Loss: 1.805383563041687\n",
      "Epoch 25, Loss: 1.5583094358444214\n",
      "Epoch 25, Loss: 1.5172836780548096\n",
      "Epoch 26, Loss: 1.9755749702453613\n",
      "Epoch 26, Loss: 1.7124252319335938\n",
      "Epoch 26, Loss: 1.511664867401123\n",
      "Epoch 26, Loss: 1.3494561910629272\n",
      "Epoch 27, Loss: 1.8851910829544067\n",
      "Epoch 27, Loss: 1.6995924711227417\n",
      "Epoch 27, Loss: 1.4888300895690918\n",
      "Epoch 27, Loss: 1.3158105611801147\n",
      "Epoch 28, Loss: 1.7586925029754639\n",
      "Epoch 28, Loss: 1.610653042793274\n",
      "Epoch 28, Loss: 1.4339596033096313\n",
      "Epoch 28, Loss: 1.325057864189148\n",
      "Epoch 29, Loss: 1.758499264717102\n",
      "Epoch 29, Loss: 1.6078354120254517\n",
      "Epoch 29, Loss: 1.3738216161727905\n",
      "Epoch 29, Loss: 1.1484417915344238\n",
      "Epoch 30, Loss: 1.6503026485443115\n",
      "Epoch 30, Loss: 1.4781816005706787\n",
      "Epoch 30, Loss: 1.2572472095489502\n",
      "Epoch 30, Loss: 1.0753337144851685\n",
      "Epoch 31, Loss: 1.5820672512054443\n",
      "Epoch 31, Loss: 1.5327389240264893\n",
      "Epoch 31, Loss: 1.2127131223678589\n",
      "Epoch 31, Loss: 0.856126606464386\n",
      "Epoch 32, Loss: 1.5543334484100342\n",
      "Epoch 32, Loss: 1.3836536407470703\n",
      "Epoch 32, Loss: 1.182692289352417\n",
      "Epoch 32, Loss: 0.9426473379135132\n",
      "Epoch 33, Loss: 1.491956114768982\n",
      "Epoch 33, Loss: 1.234326720237732\n",
      "Epoch 33, Loss: 1.090437412261963\n",
      "Epoch 33, Loss: 0.8753350377082825\n",
      "Epoch 34, Loss: 1.3753504753112793\n",
      "Epoch 34, Loss: 1.2453209161758423\n",
      "Epoch 34, Loss: 1.0812605619430542\n",
      "Epoch 34, Loss: 0.9085615873336792\n",
      "Epoch 35, Loss: 1.4248135089874268\n",
      "Epoch 35, Loss: 1.2210748195648193\n",
      "Epoch 35, Loss: 0.9679988622665405\n",
      "Epoch 35, Loss: 0.8059594631195068\n",
      "Epoch 36, Loss: 1.364730715751648\n",
      "Epoch 36, Loss: 1.1725389957427979\n",
      "Epoch 36, Loss: 1.004176139831543\n",
      "Epoch 36, Loss: 0.7707140445709229\n",
      "Epoch 37, Loss: 1.255478024482727\n",
      "Epoch 37, Loss: 1.1065990924835205\n",
      "Epoch 37, Loss: 0.9181908965110779\n",
      "Epoch 37, Loss: 0.604248046875\n",
      "Epoch 38, Loss: 1.2370511293411255\n",
      "Epoch 38, Loss: 1.1180058717727661\n",
      "Epoch 38, Loss: 0.8380765914916992\n",
      "Epoch 38, Loss: 0.5838616490364075\n",
      "Epoch 39, Loss: 1.1004209518432617\n",
      "Epoch 39, Loss: 0.97255539894104\n",
      "Epoch 39, Loss: 0.8112757205963135\n",
      "Epoch 39, Loss: 0.5103777647018433\n",
      "Epoch 40, Loss: 1.1728521585464478\n",
      "Epoch 40, Loss: 0.9423593878746033\n",
      "Epoch 40, Loss: 0.733221709728241\n",
      "Epoch 40, Loss: 0.4857417941093445\n",
      "Epoch 41, Loss: 1.1199041604995728\n",
      "Epoch 41, Loss: 0.9373924732208252\n",
      "Epoch 41, Loss: 0.6581752896308899\n",
      "Epoch 41, Loss: 0.5742990970611572\n",
      "Epoch 42, Loss: 0.9950087666511536\n",
      "Epoch 42, Loss: 0.7809153199195862\n",
      "Epoch 42, Loss: 0.7356026768684387\n",
      "Epoch 42, Loss: 0.49249112606048584\n",
      "Epoch 43, Loss: 1.0033419132232666\n",
      "Epoch 43, Loss: 0.7529784440994263\n",
      "Epoch 43, Loss: 0.6219082474708557\n",
      "Epoch 43, Loss: 0.40521225333213806\n",
      "Epoch 44, Loss: 0.939865231513977\n",
      "Epoch 44, Loss: 0.7426403760910034\n",
      "Epoch 44, Loss: 0.5168864727020264\n",
      "Epoch 44, Loss: 0.42564231157302856\n",
      "Epoch 45, Loss: 0.9462836384773254\n",
      "Epoch 45, Loss: 0.725732684135437\n",
      "Epoch 45, Loss: 0.6085779666900635\n",
      "Epoch 45, Loss: 0.33547866344451904\n",
      "Epoch 46, Loss: 0.8787698745727539\n",
      "Epoch 46, Loss: 0.6565021872520447\n",
      "Epoch 46, Loss: 0.5535867810249329\n",
      "Epoch 46, Loss: 0.3305349051952362\n",
      "Epoch 47, Loss: 0.767255961894989\n",
      "Epoch 47, Loss: 0.6399332880973816\n",
      "Epoch 47, Loss: 0.46722856163978577\n",
      "Epoch 47, Loss: 0.36609458923339844\n",
      "Epoch 48, Loss: 0.7868722081184387\n",
      "Epoch 48, Loss: 0.5769058465957642\n",
      "Epoch 48, Loss: 0.4561633765697479\n",
      "Epoch 48, Loss: 0.2858272194862366\n",
      "Epoch 49, Loss: 0.6849287152290344\n",
      "Epoch 49, Loss: 0.5236367583274841\n",
      "Epoch 49, Loss: 0.48524606227874756\n",
      "Epoch 49, Loss: 0.24367588758468628\n",
      "Epoch 50, Loss: 0.699077844619751\n",
      "Epoch 50, Loss: 0.5302640199661255\n",
      "Epoch 50, Loss: 0.4158495366573334\n",
      "Epoch 50, Loss: 0.3076843023300171\n",
      "Epoch 51, Loss: 0.676724910736084\n",
      "Epoch 51, Loss: 0.5231621861457825\n",
      "Epoch 51, Loss: 0.42474761605262756\n",
      "Epoch 51, Loss: 0.23411884903907776\n",
      "Epoch 52, Loss: 0.615290105342865\n",
      "Epoch 52, Loss: 0.4338054358959198\n",
      "Epoch 52, Loss: 0.3904920518398285\n",
      "Epoch 52, Loss: 0.3150145709514618\n",
      "Epoch 53, Loss: 0.5849223136901855\n",
      "Epoch 53, Loss: 0.49417269229888916\n",
      "Epoch 53, Loss: 0.4083746373653412\n",
      "Epoch 53, Loss: 0.24808891117572784\n",
      "Epoch 54, Loss: 0.5589718818664551\n",
      "Epoch 54, Loss: 0.48270851373672485\n",
      "Epoch 54, Loss: 0.3765355050563812\n",
      "Epoch 54, Loss: 0.20791855454444885\n",
      "Epoch 55, Loss: 0.5927440524101257\n",
      "Epoch 55, Loss: 0.39155805110931396\n",
      "Epoch 55, Loss: 0.3404947817325592\n",
      "Epoch 55, Loss: 0.1962292492389679\n",
      "Epoch 56, Loss: 0.5612374544143677\n",
      "Epoch 56, Loss: 0.3804256319999695\n",
      "Epoch 56, Loss: 0.3007929027080536\n",
      "Epoch 56, Loss: 0.17242711782455444\n",
      "Epoch 57, Loss: 0.490637868642807\n",
      "Epoch 57, Loss: 0.38313886523246765\n",
      "Epoch 57, Loss: 0.2940997779369354\n",
      "Epoch 57, Loss: 0.1471008062362671\n",
      "Epoch 58, Loss: 0.47447577118873596\n",
      "Epoch 58, Loss: 0.32599735260009766\n",
      "Epoch 58, Loss: 0.252620667219162\n",
      "Epoch 58, Loss: 0.2244086116552353\n",
      "Epoch 59, Loss: 0.4664020538330078\n",
      "Epoch 59, Loss: 0.3526170253753662\n",
      "Epoch 59, Loss: 0.28982874751091003\n",
      "Epoch 59, Loss: 0.18865354359149933\n",
      "Epoch 60, Loss: 0.4250500202178955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60, Loss: 0.3739577829837799\n",
      "Epoch 60, Loss: 0.2608049213886261\n",
      "Epoch 60, Loss: 0.18228524923324585\n",
      "Epoch 61, Loss: 0.41251930594444275\n",
      "Epoch 61, Loss: 0.3588440418243408\n",
      "Epoch 61, Loss: 0.21098600327968597\n",
      "Epoch 61, Loss: 0.24409261345863342\n",
      "Epoch 62, Loss: 0.3779027760028839\n",
      "Epoch 62, Loss: 0.40368565917015076\n",
      "Epoch 62, Loss: 0.3108478784561157\n",
      "Epoch 62, Loss: 0.18592113256454468\n",
      "Epoch 63, Loss: 0.4091744124889374\n",
      "Epoch 63, Loss: 0.33826640248298645\n",
      "Epoch 63, Loss: 0.18890583515167236\n",
      "Epoch 63, Loss: 0.1293259710073471\n",
      "Epoch 64, Loss: 0.3094351589679718\n",
      "Epoch 64, Loss: 0.3348146677017212\n",
      "Epoch 64, Loss: 0.17609691619873047\n",
      "Epoch 64, Loss: 0.1091364175081253\n",
      "Epoch 65, Loss: 0.3191952705383301\n",
      "Epoch 65, Loss: 0.2949370741844177\n",
      "Epoch 65, Loss: 0.23293346166610718\n",
      "Epoch 65, Loss: 0.19986312091350555\n",
      "Epoch 66, Loss: 0.3184839189052582\n",
      "Epoch 66, Loss: 0.2903757393360138\n",
      "Epoch 66, Loss: 0.20999859273433685\n",
      "Epoch 66, Loss: 0.24134176969528198\n",
      "Epoch 67, Loss: 0.2856663465499878\n",
      "Epoch 67, Loss: 0.2344856858253479\n",
      "Epoch 67, Loss: 0.262119323015213\n",
      "Epoch 67, Loss: 0.15953506529331207\n",
      "Epoch 68, Loss: 0.29684436321258545\n",
      "Epoch 68, Loss: 0.25275862216949463\n",
      "Epoch 68, Loss: 0.16154080629348755\n",
      "Epoch 68, Loss: 0.1661139875650406\n",
      "Epoch 69, Loss: 0.2664729654788971\n",
      "Epoch 69, Loss: 0.2341892272233963\n",
      "Epoch 69, Loss: 0.20592544972896576\n",
      "Epoch 69, Loss: 0.15135231614112854\n",
      "Epoch 70, Loss: 0.24687063694000244\n",
      "Epoch 70, Loss: 0.21025682985782623\n",
      "Epoch 70, Loss: 0.15829694271087646\n",
      "Epoch 70, Loss: 0.17397044599056244\n",
      "Epoch 71, Loss: 0.26693272590637207\n",
      "Epoch 71, Loss: 0.2133861929178238\n",
      "Epoch 71, Loss: 0.19719849526882172\n",
      "Epoch 71, Loss: 0.19711488485336304\n",
      "Epoch 72, Loss: 0.24240218102931976\n",
      "Epoch 72, Loss: 0.247336283326149\n",
      "Epoch 72, Loss: 0.14005912840366364\n",
      "Epoch 72, Loss: 0.12198547273874283\n",
      "Epoch 73, Loss: 0.2096148580312729\n",
      "Epoch 73, Loss: 0.24088606238365173\n",
      "Epoch 73, Loss: 0.13276098668575287\n",
      "Epoch 73, Loss: 0.12188024818897247\n",
      "Epoch 74, Loss: 0.20261234045028687\n",
      "Epoch 74, Loss: 0.19114436209201813\n",
      "Epoch 74, Loss: 0.12733030319213867\n",
      "Epoch 74, Loss: 0.1512855738401413\n",
      "Epoch 75, Loss: 0.192012757062912\n",
      "Epoch 75, Loss: 0.20644663274288177\n",
      "Epoch 75, Loss: 0.14100554585456848\n",
      "Epoch 75, Loss: 0.158067524433136\n",
      "Epoch 76, Loss: 0.2138337343931198\n",
      "Epoch 76, Loss: 0.25028282403945923\n",
      "Epoch 76, Loss: 0.15147265791893005\n",
      "Epoch 76, Loss: 0.05499762296676636\n",
      "Epoch 77, Loss: 0.19042783975601196\n",
      "Epoch 77, Loss: 0.16607265174388885\n",
      "Epoch 77, Loss: 0.11559651046991348\n",
      "Epoch 77, Loss: 0.06203717738389969\n",
      "Epoch 78, Loss: 0.21987852454185486\n",
      "Epoch 78, Loss: 0.16949661076068878\n",
      "Epoch 78, Loss: 0.1365501582622528\n",
      "Epoch 78, Loss: 0.08635498583316803\n",
      "Epoch 79, Loss: 0.18505394458770752\n",
      "Epoch 79, Loss: 0.18521440029144287\n",
      "Epoch 79, Loss: 0.12232156842947006\n",
      "Epoch 79, Loss: 0.10806278139352798\n",
      "Epoch 80, Loss: 0.19909536838531494\n",
      "Epoch 80, Loss: 0.13641220331192017\n",
      "Epoch 80, Loss: 0.1375674307346344\n",
      "Epoch 80, Loss: 0.08821485191583633\n",
      "Epoch 81, Loss: 0.2113170176744461\n",
      "Epoch 81, Loss: 0.2051781266927719\n",
      "Epoch 81, Loss: 0.14361746609210968\n",
      "Epoch 81, Loss: 0.14001692831516266\n",
      "Epoch 82, Loss: 0.21463030576705933\n",
      "Epoch 82, Loss: 0.12177328020334244\n",
      "Epoch 82, Loss: 0.1337829977273941\n",
      "Epoch 82, Loss: 0.1038665920495987\n",
      "Epoch 83, Loss: 0.15786194801330566\n",
      "Epoch 83, Loss: 0.15515442192554474\n",
      "Epoch 83, Loss: 0.12093570083379745\n",
      "Epoch 83, Loss: 0.05882658064365387\n",
      "Epoch 84, Loss: 0.1533568799495697\n",
      "Epoch 84, Loss: 0.1255771815776825\n",
      "Epoch 84, Loss: 0.12268398702144623\n",
      "Epoch 84, Loss: 0.08561451733112335\n",
      "Epoch 85, Loss: 0.1487497091293335\n",
      "Epoch 85, Loss: 0.12732097506523132\n",
      "Epoch 85, Loss: 0.11596375703811646\n",
      "Epoch 85, Loss: 0.04870377108454704\n",
      "Epoch 86, Loss: 0.15483200550079346\n",
      "Epoch 86, Loss: 0.164583221077919\n",
      "Epoch 86, Loss: 0.12732714414596558\n",
      "Epoch 86, Loss: 0.13183723390102386\n",
      "Epoch 87, Loss: 0.12716229259967804\n",
      "Epoch 87, Loss: 0.21865305304527283\n",
      "Epoch 87, Loss: 0.1459183245897293\n",
      "Epoch 87, Loss: 0.1388409435749054\n",
      "Epoch 88, Loss: 0.1485053151845932\n",
      "Epoch 88, Loss: 0.16206461191177368\n",
      "Epoch 88, Loss: 0.08511890470981598\n",
      "Epoch 88, Loss: 0.06952602416276932\n",
      "Epoch 89, Loss: 0.13823971152305603\n",
      "Epoch 89, Loss: 0.10674789547920227\n",
      "Epoch 89, Loss: 0.14193151891231537\n",
      "Epoch 89, Loss: 0.08032199740409851\n",
      "Epoch 90, Loss: 0.13666914403438568\n",
      "Epoch 90, Loss: 0.07965254783630371\n",
      "Epoch 90, Loss: 0.07953577488660812\n",
      "Epoch 90, Loss: 0.055200327187776566\n",
      "Epoch 91, Loss: 0.17899461090564728\n",
      "Epoch 91, Loss: 0.14645229279994965\n",
      "Epoch 91, Loss: 0.130119189620018\n",
      "Epoch 91, Loss: 0.07092254608869553\n",
      "Epoch 92, Loss: 0.1625654399394989\n",
      "Epoch 92, Loss: 0.12025833129882812\n",
      "Epoch 92, Loss: 0.10281754285097122\n",
      "Epoch 92, Loss: 0.061351120471954346\n",
      "Epoch 93, Loss: 0.1880733072757721\n",
      "Epoch 93, Loss: 0.10009806603193283\n",
      "Epoch 93, Loss: 0.08845995366573334\n",
      "Epoch 93, Loss: 0.04570867121219635\n",
      "Epoch 94, Loss: 0.15295003354549408\n",
      "Epoch 94, Loss: 0.12416363507509232\n",
      "Epoch 94, Loss: 0.11495056003332138\n",
      "Epoch 94, Loss: 0.09249985963106155\n",
      "Epoch 95, Loss: 0.11646711826324463\n",
      "Epoch 95, Loss: 0.1427495777606964\n",
      "Epoch 95, Loss: 0.0960616022348404\n",
      "Epoch 95, Loss: 0.07264242321252823\n",
      "Epoch 96, Loss: 0.1402793973684311\n",
      "Epoch 96, Loss: 0.16483303904533386\n",
      "Epoch 96, Loss: 0.06177286058664322\n",
      "Epoch 96, Loss: 0.05027531459927559\n",
      "Epoch 97, Loss: 0.1034945398569107\n",
      "Epoch 97, Loss: 0.14140236377716064\n",
      "Epoch 97, Loss: 0.0706639438867569\n",
      "Epoch 97, Loss: 0.08457209914922714\n",
      "Epoch 98, Loss: 0.11253058910369873\n",
      "Epoch 98, Loss: 0.0765744224190712\n",
      "Epoch 98, Loss: 0.07898686081171036\n",
      "Epoch 98, Loss: 0.10778941959142685\n",
      "Epoch 99, Loss: 0.10783902555704117\n",
      "Epoch 99, Loss: 0.08895746618509293\n",
      "Epoch 99, Loss: 0.06912457197904587\n",
      "Epoch 99, Loss: 0.06336651742458344\n",
      "Epoch 100, Loss: 0.08651718497276306\n",
      "Epoch 100, Loss: 0.08288802951574326\n",
      "Epoch 100, Loss: 0.08276624232530594\n",
      "Epoch 100, Loss: 0.0938902497291565\n",
      "Model saved to: fine_tuned_model\n"
     ]
    }
   ],
   "source": [
    "##Section 1:\n",
    "import fitz  # PyMuPDF\n",
    "import re\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Function to chunk text into smaller parts\n",
    "def chunk_text(text, chunk_size=1024):\n",
    "    chunks = []\n",
    "    for i in range(0, len(text), chunk_size):\n",
    "        chunks.append(text[i:i+chunk_size])\n",
    "    return chunks\n",
    "\n",
    "# Step 1: Extract text from PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc.load_page(page_num)\n",
    "        text += page.get_text()\n",
    "    doc.close()\n",
    "    return text\n",
    "\n",
    "# Step 2: Preprocess text\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with single space\n",
    "    text = text.strip()  # Remove leading and trailing whitespace\n",
    "    return text\n",
    "\n",
    "# Step 3: Fine-tune GPT-2 model\n",
    "def fine_tune_gpt2(chunks):\n",
    "    # Load pre-trained model and tokenizer\n",
    "    model_name = 'gpt2'\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "    # Fine-tuning the model\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "    for epoch in range(100):  # Adjust number of epochs as needed\n",
    "        for chunk in chunks:\n",
    "            input_ids = tokenizer.encode(chunk, return_tensors='pt')\n",
    "            outputs = model(input_ids, labels=input_ids)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
    "\n",
    "    # Save the fine-tuned model\n",
    "    model.save_pretrained('fine_tuned_model')\n",
    "\n",
    "    return 'fine_tuned_model'  # Return the path to the saved model\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace with your PDF file path\n",
    "    pdf_path = 'EVB_Nissan_2013.pdf'\n",
    "\n",
    "    # Step 1: Extract text from PDF\n",
    "    pdf_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "    # Step 2: Preprocess text\n",
    "    preprocessed_text = preprocess_text(pdf_text)\n",
    "\n",
    "    # Step 2.1: Chunk text into smaller parts\n",
    "    text_chunks = chunk_text(preprocessed_text)\n",
    "\n",
    "    # Step 3: Fine-tune GPT-2 model on text chunks\n",
    "    model_path = fine_tune_gpt2(text_chunks)\n",
    "\n",
    "    print(f\"Model saved to: {model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1971881f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how to remove service plug? what are the warnings and dangers? • Never put on insulating coat over fire • Always put on insulating coat in tool box • Always remove cover from fire • Always put on insulating coat after every trip to vehicle floor • Always put on insulating coat after every trip to vehicle floor • Always replace after every trip to vehicle wash after every trip to dry bench • Always put on insulating coat in tool box after every trip to dry bench • Always replace after every\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Config\n",
    "\n",
    "# Example usage after fine-tuning\n",
    "if __name__ == \"__main__\":\n",
    "    # Load fine-tuned model and tokenizer configuration\n",
    "    model_path = 'fine_tuned_model.pth'  # Update with the path to your fine-tuned model .pth file\n",
    "    \n",
    "    # Load tokenizer configuration\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "    # Load model configuration\n",
    "    config = GPT2Config.from_pretrained('gpt2')\n",
    "    model = GPT2LMHeadModel(config)\n",
    "\n",
    "    # Load the state_dict from .pth file\n",
    "    state_dict = torch.load(model_path)\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "    # Example: Generate text\n",
    "    input_text = \"how to remove service plug? what are the warnings and dangers?\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "    generated = model.generate(input_ids, max_length=100, num_return_sequences=1)\n",
    "    decoded_text = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "    print(decoded_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d06683",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819aaf55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
